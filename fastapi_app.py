#!/usr/bin/env python3
# Generated by Claude Code
"""
FastAPI entrypoint for krknctl-compatible RAG service
Uses the new FAISS + llama.cpp + all-MiniLM-L6-v2 pipeline
Compatible with krknctl OpenAI-style API
"""

import logging
import time
import re
from contextlib import asynccontextmanager
from typing import List, Dict, Any, Optional

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from rag_pipelines.llama31_krknctl_rag_pipeline import (
    load_llama31_krknctl_rag_pipeline,
)

# Configure logging first
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
try:
    from utils.state_graph import get_context  # NOQA
except ImportError:
    logger.warning(
        "utils.state_graph not available, "
        "sources formatting will be simplified"
    )

    # Optional import for backward compatibility

    def get_context(result):
        """Fallback implementation when langgraph is not available"""
        context_docs = result.get("context", [])
        sources_md_lines = []
        if context_docs:
            sources_md_lines.append("\nSources and context:")
            for idx, doc in enumerate(context_docs, start=1):
                title = doc.metadata.get("title", "unknown")
                url = doc.metadata.get("url", "")
                snippet = doc.page_content.strip().replace("\n", " ")
                if len(snippet) > 300:
                    snippet = snippet[:300].rstrip() + "..."
                sources_md_lines.append(f"  {idx}. {title}")
                if url:
                    sources_md_lines.append(f"     {url}")
                sources_md_lines.append(f"     {snippet}")
        return sources_md_lines


# Global pipeline instance
rag_pipeline = None


# krknctl-compatible models (matching OpenAI format)
class ChatMessage(BaseModel):
    role: str
    content: str


class QueryRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: Optional[float] = 0.1
    max_tokens: Optional[int] = 512
    stream: Optional[bool] = False


class QueryChoice(BaseModel):
    index: int
    message: ChatMessage
    finish_reason: str


class Usage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


class QueryResponse(BaseModel):
    id: str
    object: str
    created: int
    model: str
    choices: List[QueryChoice]
    usage: Usage
    scenario_name: Optional[str] = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage application lifespan (startup and shutdown)"""
    global rag_pipeline

    # Startup
    logger.info("Starting krknctl RAG service with FAISS backend...")
    try:
        # Load the new pipeline with enhanced krkn-hub scenario indexing
        rag_pipeline = load_llama31_krknctl_rag_pipeline(
            github_repo="https://github.com/krkn-chaos/website",
            repo_path="content/en/docs",
            persist_dir="faiss_index",
            model_path="models/Llama-3.2-3B-Instruct-Q4_K_M.gguf",
            krkn_hub_repo="https://github.com/krkn-chaos/krkn-hub",
        )
        logger.info("krknctl RAG pipeline initialized successfully")
    except Exception as e:
        logger.error(f"Failed to initialize RAG pipeline: {e}")
        raise

    yield

    # Shutdown
    logger.info("krknctl RAG service shutting down...")


# Create FastAPI app with lifespan management
app = FastAPI(
    title="krknctl Lightspeed RAG Service",
    version="2.0.0",
    description="Retrieval-Augmented Generation service for "
    "krknctl chaos engineering assistance using FAISS + llama.cpp",
    lifespan=lifespan,
)


def extract_scenario_name(response_text: str) -> Optional[str]:
    """Extract scenario name from response if SCENARIO: tag is present"""
    # More flexible regex that captures scenario names with dashes,
    # optionally wrapped in quotes, backticks, or other symbols
    match = re.search(
        r'SCENARIO:\s*[`"\']*([a-zA-Z0-9][a-zA-Z0-9\-_]*'
        r'[a-zA-Z0-9]|[a-zA-Z0-9]+)[`"\']*',
        response_text,
    )
    if match:
        return match.group(1).strip()
    return None


def get_user_query_from_messages(messages: List[ChatMessage]) -> str:
    """Extract user query from OpenAI messages format"""
    # Get the last user message
    for message in reversed(messages):
        if message.role == "user":
            return message.content
    return ""


@app.get("/")
async def root():
    """Root endpoint with service information"""
    return {
        "service": "krknctl Lightspeed RAG",
        "version": "2.0.0",
        "backend": "FAISS + llama.cpp + all-MiniLM-L6-v2",
        "description": "AI-powered assistance for "
        "krknctl chaos engineering scenarios",
        "endpoints": {"health": "/health", "query": "/v1/chat/completions"},
    }


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    if not rag_pipeline:
        raise HTTPException(
            status_code=503, detail="RAG pipeline not initialized"
        )

    # Get document count from pipeline
    documents_count = 0
    try:
        if hasattr(rag_pipeline, "get_documents_count"):
            documents_count = rag_pipeline.get_documents_count()
        else:
            logger.warning("Pipeline does not have get_documents_count method")
    except Exception as e:
        logger.warning(f"Could not get document count: {e}")

    return {
        "status": "healthy",
        "service": "krknctl-lightspeed-rag",
        "model": "Llama-3.2-3B-Instruct-Q4_K_M",
        "model_status": "loaded",
        "documents_indexed": documents_count,
    }


@app.post("/v1/chat/completions", response_model=QueryResponse)
async def chat_completions(request: QueryRequest):
    """
    OpenAI-compatible chat completions endpoint
    Compatible with krknctl's expected API format
    """
    try:
        if not rag_pipeline:
            raise HTTPException(
                status_code=503, detail="RAG pipeline not initialized"
            )

        if request.stream:
            raise HTTPException(
                status_code=400, detail="Streaming not yet supported"
            )

        # Extract user query from messages
        user_query = get_user_query_from_messages(request.messages)
        if not user_query:
            raise HTTPException(
                status_code=400, detail="No user message found in request"
            )

        # Execute the pipeline
        result = rag_pipeline.invoke({"question": user_query})

        # Extract response and context
        response_text = result.get("answer", "No response generated")

        # Try to extract scenario name from response
        scenario_name = extract_scenario_name(response_text)

        # Create OpenAI-compatible response
        current_time = int(time.time())
        response_id = f"chatcmpl-{current_time}"

        # Estimate token usage (rough approximation)
        prompt_tokens = sum(
            len(msg.content.split()) for msg in request.messages
        )
        completion_tokens = len(response_text.split())
        total_tokens = prompt_tokens + completion_tokens

        return QueryResponse(
            id=response_id,
            object="chat.completion",
            created=current_time,
            model=request.model or "llama-3.2-3b-instruct",
            choices=[
                QueryChoice(
                    index=0,
                    message=ChatMessage(
                        role="assistant", content=response_text
                    ),
                    finish_reason="stop",
                )
            ],
            usage=Usage(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=total_tokens,
            ),
            scenario_name=scenario_name,
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing chat completion: {e}")
        raise HTTPException(
            status_code=500, detail=f"Internal server error: {str(e)}"
        )


# Legacy endpoints for backward compatibility
@app.post("/query")
async def legacy_query(request: Dict[str, Any]):
    """
    Legacy query endpoint for backward compatibility
    Converts to OpenAI format and calls chat_completions
    """
    try:
        # Convert legacy format to OpenAI format
        query = request.get("query", "")
        if not query:
            raise HTTPException(
                status_code=400, detail="Missing 'query' field"
            )

        openai_request = QueryRequest(
            model="llama-3.2-3b-instruct",
            messages=[ChatMessage(role="user", content=query)],
            temperature=request.get("temperature", 0.1),
            max_tokens=request.get("max_tokens", 512),
            stream=request.get("stream", False),
        )

        # Call the main endpoint
        response = await chat_completions(openai_request)

        # Convert back to legacy format
        return {
            "response": response.choices[0].message.content,
            "sources": [],  # Legacy format doesn't include detailed sources
            "query": query,
            "scenario_name": response.scenario_name,
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in legacy query endpoint: {e}")
        raise HTTPException(
            status_code=500, detail=f"Internal server error: {str(e)}"
        )


@app.post("/chat")
async def chat_with_krknctl(request: Dict[str, Any]):
    """
    Chat endpoint compatible with existing krkn-lightspeed structure
    Returns response in the format expected by existing UI components
    """
    try:
        if not rag_pipeline:
            raise HTTPException(
                status_code=503, detail="RAG pipeline not initialized"
            )

        query = request.get("query", "")
        if not query:
            raise HTTPException(
                status_code=400, detail="Missing 'query' field"
            )

        # Execute the pipeline
        result = rag_pipeline.invoke({"question": query})

        # Format response similar to existing state_graph format
        response_data = {
            "answer": result.get("answer", "No response generated"),
            "context": result.get("context", []),
            "question": query,
        }

        # Add sources information using existing utility function
        try:
            sources_md_lines = get_context(result)
            sources_md = "\n\n".join(sources_md_lines)
            if sources_md:
                response_data["sources_markdown"] = sources_md
        except Exception as e:
            logger.warning(f"Could not format sources: {e}")

        return response_data

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in chat endpoint: {e}")
        raise HTTPException(
            status_code=500, detail=f"Internal server error: {str(e)}"
        )


if __name__ == "__main__":
    import uvicorn

    logger.info("Starting krknctl Lightspeed RAG service...")
    uvicorn.run(app, host="0.0.0.0", port=8080, log_level="info")
