# Generated by Claude Code
"""
krknctl-compatible RAG pipeline using FAISS + llama.cpp + all-MiniLM-L6-v2
Integrates with existing krkn-lightspeed structure
while using our optimized indexing system
"""

import os
import logging
from llama_cpp import Llama
from utils.faiss_document_indexer import FaissDocumentIndexer
from utils.faiss_vector_store import FAISSVectorStore, SimpleStateGraph
from utils.state_graph import State

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def detect_gpu_type():
    """Detect the type of GPU available and return
    appropriate llama.cpp parameters"""
    logger.info("Detecting GPU type for optimal llama.cpp backend...")

    # Check for NVIDIA GPU devices
    if os.path.exists("/dev/nvidia0") or os.path.exists("/dev/nvidiactl"):
        logger.info("NVIDIA GPU devices detected - using CUDA backend")
        return {
            "backend": "cuda",
            "n_gpu_layers": -1,
            "main_gpu": 0,
            "verbose": False,
        }

    # Check for DRI devices (Apple Silicon or other GPUs with Vulkan)
    if os.path.exists("/dev/dri/card0") or os.path.exists(
        "/dev/dri/renderD128"
    ):
        logger.info("DRI devices detected - using Vulkan backend")
        return {"backend": "vulkan", "n_gpu_layers": -1, "verbose": False}

    # Fallback to CPU
    logger.info("No GPU detected - using CPU backend")
    return {
        "backend": "cpu",
        "n_gpu_layers": 0,
        "n_threads": os.cpu_count(),
        "verbose": False,
    }


def load_llama31_krknctl_rag_pipeline(
    github_repo="https://github.com/krkn-chaos/website",
    repo_path="content/en/docs",
    persist_dir="faiss_index",
    model_path="models/Llama-3.2-3B-Instruct-Q4_K_M.gguf",
    krkn_hub_repo="https://github.com/krkn-chaos/krkn-hub",
):
    """
    Load krknctl RAG pipeline using FAISS + llama.cpp + all-MiniLM-L6-v2

    Compatible with existing krkn-lightspeed state graph structure
    """

    logger.info("Loading krknctl RAG pipeline with FAISS backend")

    # Ensure directories exist
    os.makedirs(persist_dir, exist_ok=True)
    os.makedirs(os.path.dirname(model_path), exist_ok=True)

    # Check if index exists, if not build it
    index_path = os.path.join(persist_dir, "index.faiss")
    if not os.path.exists(index_path):
        logger.info("Index not found, building new index...")
        indexer = FaissDocumentIndexer()
        indexer.build_and_save_index(
            github_repo, repo_path, persist_dir, krkn_hub_repo
        )

    # Load vector store
    logger.info("Loading FAISS vector store...")
    vector_store = FAISSVectorStore(persist_dir)

    # Initialize Llama model
    llama_model = None
    if os.path.exists(model_path):
        logger.info(f"Loading Llama 3.2 3B model from {model_path}")

        gpu_config = detect_gpu_type()
        logger.info(f"Using GPU backend: {gpu_config['backend']}")

        llama_params = {
            "model_path": model_path,
            "n_ctx": 4096,
            "n_gpu_layers": gpu_config["n_gpu_layers"],
            "verbose": gpu_config["verbose"],
        }

        if gpu_config["backend"] == "cuda":
            llama_params["main_gpu"] = gpu_config["main_gpu"]
        elif gpu_config["backend"] == "cpu":
            llama_params["n_threads"] = gpu_config["n_threads"]

        llama_model = Llama(**llama_params)
        logger.info("Llama 3.2 3B model loaded successfully")
    else:
        logger.warning(f"Llama model not found at {model_path}")

    # Build state graph compatible with existing structure
    def retrieve(state: State):
        """Retrieve relevant documents"""
        retrieved_docs = vector_store.similarity_search(state["question"], k=5)
        return {"context": retrieved_docs}

    def generate(state: State):
        """Generate response using llama.cpp with conservative parameters"""
        if not llama_model:
            return {
                "answer": "Llama model not available. Please check model path."
            }

        # Build context from retrieved documents with
        # source information for model decision-making
        context_parts = []
        for i, doc in enumerate(state["context"]):
            relevance_score = doc.metadata.get("relevance_score", 0)
            source = doc.metadata.get("source", "unknown")
            context_parts.append(
                f"[RELEVANCE: {relevance_score:.3f} | "
                f"SOURCE: {source}] {doc.page_content}"
            )

        docs_content = "\n\n".join(context_parts)

        # Create enhanced prompt with source-based guidance
        prompt = f"""You are a chaos engineering assistant. 
        Answer the user's question using the provided documentation context.

CONTEXT:
{docs_content}

QUESTION: {state["question"]}

INSTRUCTIONS:
1. Look at the SOURCE information in the context to determine response type:
   - If SOURCE contains "chaos-testing-guide": Provide theoretical chaos testing explanations
   - If SOURCE contains "krkn-hub" or "scenario": Provide operational krknctl commands with "SCENARIO: exact-scenario-name"
   - If SOURCE contains "krknctl": Provide practical command guidance

2. Use documents with higher RELEVANCE scores to prioritize information
3. For operational questions:
   - Look for "krknctl Scenario:" or "Command: krknctl run" in the documentation to find the exact scenario name
   - Extract the EXACT scenario name as written (e.g., if you see "krknctl run zone-outages", use "zone-outages" not "zone-outage")
   - Do not modify, abbreviate, or change the scenario name in any way
   - Include specific krknctl commands and required flags
   - Format: "SCENARIO: exact-scenario-name-as-documented"
4. For theoretical questions: Focus on concepts, best practices, and methodologies
5. Give a direct, professional answer without exposing the technical metadata

Answer:"""  # NOQA

        try:
            # Enhanced parameters for complete, professional responses
            response = llama_model(
                prompt,
                max_tokens=600,  # Sufficient space for complete answers
                temperature=0.1,  # Very focused and consistent
                top_p=0.9,  # High quality sampling
                repeat_penalty=1.15,
                echo=False,
            )

            generated_response = response["choices"][0]["text"].strip()

            # DEBUG: Print generated response
            print("\n[DEBUG] GENERATED RESPONSE:")
            print(f"Query: {state['question']}")
            print(f"Response length: {len(generated_response)} characters")
            print(f"Response preview: {generated_response[:200]}...")
            if "SCENARIO:" in generated_response:
                print("SCENARIO tag detected - krknctl command response")
            else:
                print(
                    "No SCENARIO tag - likely theoretical "
                    "chaos testing response"
                )
            print("[DEBUG] Response generation completed\n")

            return {"answer": generated_response}

        except Exception as e:
            logger.error(f"Error generating response: {e}")
            return {
                "answer": "I encountered an error while "
                "processing your request. Please try again."
            }

    # Create a simple state graph object that mimics langgraph behavior

    graph = SimpleStateGraph(retrieve, generate, vector_store)
    logger.info("krknctl RAG pipeline ready!")

    return graph
