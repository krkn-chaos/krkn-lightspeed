# Generated by Claude Code
"""
krknctl-compatible RAG pipeline using FAISS + llama.cpp + all-MiniLM-L6-v2
Integrates with existing krkn-lightspeed structure while using our optimized indexing system
"""

import os
import logging
import faiss
from llama_cpp import Llama
from utils.faiss_document_indexer import FaissDocumentIndexer
from utils.faiss_vector_store import FAISSVectorStore, SimpleStateGraph
from utils.state_graph import State

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def detect_gpu_type():
    """Detect the type of GPU available and return appropriate llama.cpp parameters"""
    logger.info("Detecting GPU type for optimal llama.cpp backend...")

    # Check for NVIDIA GPU devices
    if os.path.exists('/dev/nvidia0') or os.path.exists('/dev/nvidiactl'):
        logger.info("NVIDIA GPU devices detected - using CUDA backend")
        return {
            'backend': 'cuda',
            'n_gpu_layers': -1,
            'main_gpu': 0,
            'verbose': False
        }

    # Check for DRI devices (Apple Silicon or other GPUs with Vulkan)
    if os.path.exists('/dev/dri/card0') or os.path.exists('/dev/dri/renderD128'):
        logger.info("DRI devices detected - using Vulkan backend")
        return {
            'backend': 'vulkan',
            'n_gpu_layers': -1,
            'verbose': False
        }

    # Fallback to CPU
    logger.info("No GPU detected - using CPU backend")
    return {
        'backend': 'cpu',
        'n_gpu_layers': 0,
        'n_threads': os.cpu_count(),
        'verbose': False
    }

def load_llama31_krknctl_rag_pipeline(
    github_repo="https://github.com/krkn-chaos/website",
    repo_path="content/en/docs",
    persist_dir="faiss_index",
    model_path="models/Llama-3.2-3B-Instruct-Q4_K_M.gguf",
    krkn_hub_repo="https://github.com/krkn-chaos/krkn-hub"
):
    """
    Load krknctl RAG pipeline using FAISS + llama.cpp + all-MiniLM-L6-v2

    Compatible with existing krkn-lightspeed state graph structure
    """

    logger.info("Loading krknctl RAG pipeline with FAISS backend")

    # Ensure directories exist
    os.makedirs(persist_dir, exist_ok=True)
    os.makedirs(os.path.dirname(model_path), exist_ok=True)

    # Check if index exists, if not build it
    index_path = os.path.join(persist_dir, "index.faiss")
    if not os.path.exists(index_path):
        logger.info("Index not found, building new index...")
        indexer = FaissDocumentIndexer()
        indexer.build_and_save_index(github_repo, repo_path, persist_dir, krkn_hub_repo)

    # Load vector store
    logger.info("Loading FAISS vector store...")
    vector_store = FAISSVectorStore(persist_dir)

    # Initialize Llama model
    llama_model = None
    if os.path.exists(model_path):
        logger.info(f"Loading Llama 3.2 3B model from {model_path}")

        gpu_config = detect_gpu_type()
        logger.info(f"Using GPU backend: {gpu_config['backend']}")

        llama_params = {
            'model_path': model_path,
            'n_ctx': 4096,
            'n_gpu_layers': gpu_config['n_gpu_layers'],
            'verbose': gpu_config['verbose']
        }

        if gpu_config['backend'] == 'cuda':
            llama_params['main_gpu'] = gpu_config['main_gpu']
        elif gpu_config['backend'] == 'cpu':
            llama_params['n_threads'] = gpu_config['n_threads']

        llama_model = Llama(**llama_params)
        logger.info("Llama 3.2 3B model loaded successfully")
    else:
        logger.warning(f"Llama model not found at {model_path}")

    # Build state graph compatible with existing structure
    def retrieve(state: State):
        """Retrieve relevant documents"""
        retrieved_docs = vector_store.similarity_search(state["question"], k=5)
        return {"context": retrieved_docs}

    def generate(state: State):
        """Generate response using llama.cpp with conservative parameters"""
        if not llama_model:
            return {"answer": "Llama model not available. Please check model path."}

        # Build context from retrieved documents with source information for model decision-making
        context_parts = []
        for i, doc in enumerate(state["context"]):
            relevance_score = doc.metadata.get('relevance_score', 0)
            source = doc.metadata.get('source', 'unknown')
            context_parts.append(f"[RELEVANCE: {relevance_score:.3f} | SOURCE: {source}] {doc.page_content}")

        docs_content = "\n\n".join(context_parts)

        # Create enhanced prompt with source-based guidance
        prompt = f"""You are a chaos engineering assistant. Answer the user's question using the provided documentation context.

CONTEXT:
{docs_content}

QUESTION: {state["question"]}

INSTRUCTIONS:
1. Look at the SOURCE information in the context to determine response type:
   - If SOURCE contains "chaos-testing-guide": Provide theoretical chaos testing explanations
   - If SOURCE contains "krkn-hub" or "scenario": Provide operational krknctl commands with "SCENARIO: exact-scenario-name"
   - If SOURCE contains "krknctl": Provide practical command guidance

2. Use documents with higher RELEVANCE scores to prioritize information
3. For operational questions:
   - Look for "krknctl Scenario:" or "Command: krknctl run" in the documentation to find the exact scenario name
   - Extract the EXACT scenario name as written (e.g., if you see "krknctl run zone-outages", use "zone-outages" not "zone-outage")
   - Do not modify, abbreviate, or change the scenario name in any way
   - Include specific krknctl commands and required flags
   - Format: "SCENARIO: exact-scenario-name-as-documented"
4. For theoretical questions: Focus on concepts, best practices, and methodologies
5. Give a direct, professional answer without exposing the technical metadata

Answer:"""

        try:
            # Enhanced parameters for complete, professional responses
            response = llama_model(
                prompt,
                max_tokens=600,   # Sufficient space for complete answers
                temperature=0.1,  # Very focused and consistent
                top_p=0.9,        # High quality sampling
                repeat_penalty=1.15,
                echo=False
            )

            generated_response = response['choices'][0]['text'].strip()

            # DEBUG: Print generated response
            print(f"\n[DEBUG] GENERATED RESPONSE:")
            print(f"Query: {state['question']}")
            print(f"Response length: {len(generated_response)} characters")
            print(f"Response preview: {generated_response[:200]}...")
            if "SCENARIO:" in generated_response:
                print(f"SCENARIO tag detected - krknctl command response")
            else:
                print(f"No SCENARIO tag - likely theoretical chaos testing response")
            print("[DEBUG] Response generation completed\n")

            return {"answer": generated_response}

        except Exception as e:
            logger.error(f"Error generating response: {e}")
            return {"answer": f"I encountered an error while processing your request. Please try again."}

    # Create a simple state graph object that mimics langgraph behavior


    graph = SimpleStateGraph(retrieve, generate, vector_store)
    logger.info("krknctl RAG pipeline ready!")

    return graph