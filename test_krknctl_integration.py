#!/usr/bin/env python3
# Generated by Claude Code
"""
Test script for krknctl-compatible FAISS + llama.cpp RAG pipeline
Verifies that the new implementation works correctly
"""

import os
import sys
import time
import logging

# Add current directory to path for imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

try:
    from rag_pipelines.llama31_krknctl_rag_pipeline import (
        load_llama31_krknctl_rag_pipeline,
    )
    from utils.state_graph import get_context  # NOQA
except ImportError as e:
    print(f"Import error: {e}")
    print(
        "Make sure you've installed the dependencies "
        "from requirements_krknctl.txt:"
    )
    print("pip install -r requirements_krknctl.txt")
    sys.exit(1)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_pipeline_loading():
    """Test that the pipeline loads correctly"""
    print("\n=== Testing Pipeline Loading ===")

    try:
        pipeline = load_llama31_krknctl_rag_pipeline(
            github_repo="https://github.com/krkn-chaos/website",
            repo_path="content/en/docs",
            persist_dir="test_faiss_index",
            model_path="models/Llama-3.2-3B-Instruct-Q4_K_M.gguf",
        )
        print("‚úÖ Pipeline loaded successfully")
        return pipeline
    except Exception as e:
        print(f"‚ùå Pipeline loading failed: {e}")
        return None


def test_query_execution(pipeline):
    """Test query execution with the pipeline"""
    print("\n=== Testing Query Execution ===")

    test_queries = [
        "What is krknctl?",
        "How do I run a pod kill scenario?",
        "What scenarios are available for chaos engineering?",
        "Tell me about node scenarios",
    ]

    for query in test_queries:
        print(f"\nQuery: {query}")
        try:
            start_time = time.time()
            result = pipeline.invoke({"question": query})
            duration = time.time() - start_time

            answer = result.get("answer", "No answer")
            context_docs = result.get("context", [])

            print(f"‚úÖ Query executed in {duration:.2f}s")
            print(f"Answer length: {len(answer)} characters")
            print(f"Context documents: {len(context_docs)}")
            print(f"Answer preview: {answer[:200]}...")

            # Check for SCENARIO tag
            if "SCENARIO:" in answer:
                print("üéØ SCENARIO tag detected in response")

        except Exception as e:
            print(f"‚ùå Query failed: {e}")


def test_openai_compatibility():
    """Test OpenAI-compatible request/response format"""
    print("\n=== Testing OpenAI Compatibility ===")

    # Test data structures
    try:
        # Simulate the structures used in krknctl
        chat_message = {"role": "user", "content": "What is krknctl used for?"}

        query_request = {
            "model": "llama-3.2-3b-instruct",
            "messages": [chat_message],
            "temperature": 0.1,
            "max_tokens": 512,
            "stream": False,
        }

        print("‚úÖ OpenAI-compatible request structure created")
        print(f"Request format: {list(query_request.keys())}")

        # Test response structure
        query_response = {
            "id": "chatcmpl-test",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": "llama-3.2-3b-instruct",
            "choices": [
                {
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": "Test response",
                    },
                    "finish_reason": "stop",
                }
            ],
            "usage": {
                "prompt_tokens": 10,
                "completion_tokens": 5,
                "total_tokens": 15,
            },
            "scenario_name": None,
        }

        print("‚úÖ OpenAI-compatible response structure created")
        print(f"Response format: {list(query_response.keys())}")

    except Exception as e:
        print(f"‚ùå OpenAI compatibility test failed: {e}")


def test_scenario_detection():
    """Test scenario name detection from responses"""
    print("\n=== Testing Scenario Detection ===")

    test_responses = [
        "You can use SCENARIO: pod-kill to kill pods in your cluster",
        "Try running SCENARIO: node-cpu-hog for CPU stress testing",
        "The SCENARIO: memory-hog scenario will consume memory",
        "This is a general response without any scenario",
    ]

    import re

    for response in test_responses:
        match = re.search(r"SCENARIO:\s*([a-zA-Z0-9\-_]+)", response)
        scenario_name = match.group(1).strip() if match else None

        print(f"Response: {response[:50]}...")
        print(f"Detected scenario: {scenario_name or 'None'}")
        print("---")


def main():
    """Run all tests"""
    print("üöÄ Starting krknctl RAG Pipeline Integration Tests")

    # Test 1: Pipeline Loading
    pipeline = test_pipeline_loading()
    if not pipeline:
        print("‚ùå Cannot proceed with tests - pipeline loading failed")
        sys.exit(1)

    # Test 2: Query Execution
    test_query_execution(pipeline)

    # Test 3: OpenAI Compatibility
    test_openai_compatibility()

    # Test 4: Scenario Detection
    test_scenario_detection()

    print("\nüéâ All tests completed!")
    print("\nNext steps:")
    print(
        "1. Download Llama 3.2 3B model to "
        "models/Llama-3.2-3B-Instruct-Q4_K_M.gguf"
    )
    print("2. Run the FastAPI server: python fastapi_app.py")
    print("3. Test the API endpoints:")
    print("   - GET /health")
    print("   - POST /v1/chat/completions")
    print("   - POST /query (legacy)")
    print("   - POST /chat (compatibility)")


if __name__ == "__main__":
    main()
